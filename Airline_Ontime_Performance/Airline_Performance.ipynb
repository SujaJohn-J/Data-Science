{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6XqHi-Hb25o",
        "outputId": "34df9c9b-df9c-47a6-849f-18165adfcd66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgDpxCXUb_fI",
        "outputId": "e7820a5a-89e1-406a-ca3c-7a62d8fe7454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install flask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_jthd_ScRHg"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "import json\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Flights\").getOrCreate()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO5EZ1jZlR2w"
      },
      "source": [
        "# **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T-qfbmkcuSp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a595fac-2542-4ec0-84fb-66b5907168d7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o376.csv.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-0f7087249672>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_1991\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1991.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_2001\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2001.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_1991\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_2001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o376.csv.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ],
      "source": [
        "# Loading data\n",
        "data_1991 = spark.read.csv(\"1991.csv\", header=True, inferSchema=True)\n",
        "data_2001 = spark.read.csv(\"2001.csv\", header=True, inferSchema=True)\n",
        "data=data_1991.union(data_2001)\n",
        "data.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((data_1991.count(), len(data.columns)))\n"
      ],
      "metadata": {
        "id": "OoTGnbUfdMoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((data_2001.count(), len(data.columns)))\n"
      ],
      "metadata": {
        "id": "SAShhjERdTEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the tables in catalog\n",
        "print(spark.catalog.listTables())\n",
        "\n",
        "# adding data into spark view for sql querying\n",
        "data_1991.createOrReplaceTempView('flights_data1991')\n",
        "data_2001.createOrReplaceTempView('flights_data2001')\n",
        "\n",
        "# print the tables in catalog\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "id": "VERDUH6Udjeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"SELECT UniqueCarrier,COUNT(*) as N  FROM flights_data1991 WHERE Arrdelay<15  GROUP BY UniqueCarrier ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "arrontime"
      ],
      "metadata": {
        "id": "GHaAPNQfefJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"SELECT UniqueCarrier,COUNT(*) as N  FROM flights_data2001 WHERE Arrdelay<15  GROUP BY UniqueCarrier ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "arrontime"
      ],
      "metadata": {
        "id": "714wCyeResKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"SELECT Origin,Dest,COUNT(*) as N  FROM flights_data1991 WHERE Depdelay<15  GROUP BY Origin,Dest ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime= spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "arrontime.head(25)\n"
      ],
      "metadata": {
        "id": "9YPJiP0ueUi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"SELECT Origin,Dest,COUNT(*) as N  FROM flights_data2001 WHERE Depdelay<15 GROUP BY Origin,Dest ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime= spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "arrontime.head(25)\n"
      ],
      "metadata": {
        "id": "VHOeWUoOeX52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"SELECT DayofWeek,COUNT(*) as N  FROM flights_data1991 WHERE Arrdelay<15  GROUP BY DayofWeek ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "arrontime.head(25)"
      ],
      "metadata": {
        "id": "bGlgsrDWejvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"SELECT DayofWeek,COUNT(*) as N  FROM flights_data2001 WHERE Arrdelay<15  GROUP BY DayofWeek ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "arrontime.head(25)"
      ],
      "metadata": {
        "id": "80c8vBkaekvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"SELECT Month,COUNT(*) as N  FROM flights_data1991 WHERE Arrdelay<15 GROUP BY Month ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "arrontime.head(25)"
      ],
      "metadata": {
        "id": "pKwvAeE5e2CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"SELECT Month,COUNT(*) as N  FROM flights_data2001 WHERE Arrdelay<15 GROUP BY Month ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "arrontime.head(25)"
      ],
      "metadata": {
        "id": "IeRZ_n39e3Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"SELECT Distance,COUNT(*) as N  FROM flights_data1991 WHERE Arrdelay<15  GROUP BY Distance ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "print(arrontime.head(25))\n",
        "arrontime.tail(25)"
      ],
      "metadata": {
        "id": "3-m8ohnmfFBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"SELECT Distance,COUNT(*) as N  FROM flights_data2001 WHERE Arrdelay<15  GROUP BY Distance ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "print(arrontime.head(25))\n",
        "arrontime.tail(25)"
      ],
      "metadata": {
        "id": "R4PLXGYSfFOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSxmB3Lpl-i3"
      },
      "source": [
        "**EDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq9ESG5ftF38"
      },
      "outputs": [],
      "source": [
        "data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fJSmynhdD5X"
      },
      "outputs": [],
      "source": [
        "# Data schema\n",
        "#data.printSchema()\n",
        "\n",
        "# Checking null values\n",
        "#data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V09JjXGG1hlp"
      },
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "#data.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rulzUitw4ZCG"
      },
      "outputs": [],
      "source": [
        "print((data.count(), len(data.columns)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_hG3FZY4v1l"
      },
      "outputs": [],
      "source": [
        "# print the tables in catalog\n",
        "print(spark.catalog.listTables())\n",
        "\n",
        "# adding data into spark view for sql querying\n",
        "data.createOrReplaceTempView('flights_data')\n",
        "\n",
        "# print the tables in catalog\n",
        "print(spark.catalog.listTables())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjalotoC_twH"
      },
      "outputs": [],
      "source": [
        "query = \"SELECT Origin, Dest, COUNT(*) as N FROM flights_data GROUP BY Origin, Dest ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_counts = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "pd_counts = flight_counts.toPandas()\n",
        "\n",
        "# Print the head of pd_counts\n",
        "print(pd_counts.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai6Q-pjNVaE7"
      },
      "outputs": [],
      "source": [
        "query = \"SELECT Origin,Dest,COUNT(*) as N  FROM flights_data WHERE Arrdelay<15  GROUP BY Origin,Dest ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime= spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "arrontime.head(25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTdAHEw_IUbj"
      },
      "outputs": [],
      "source": [
        "query = \"SELECT DayofWeek,COUNT(*) as N  FROM flights_data WHERE Arrdelay<15  GROUP BY DayofWeek ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "arrontime.head(25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2_Xe1BMz5az"
      },
      "outputs": [],
      "source": [
        "query = \"SELECT Month,COUNT(*) as N  FROM flights_data WHERE Arrdelay<15 GROUP BY Month ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "arrontime.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFRaAgGHk6ZI"
      },
      "outputs": [],
      "source": [
        "query = \"SELECT Distance,COUNT(*) as N  FROM flights_data WHERE Arrdelay<15  GROUP BY Distance ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "print(arrontime.head(25))\n",
        "arrontime.tail(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUrskljBAzPk"
      },
      "outputs": [],
      "source": [
        "query = \"SELECT TailNum,COUNT(*)as N FROM flights_data WHERE ArrDelay<15 GROUP BY TailNum\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "arrontime =flight_arrontime.toPandas()\n",
        "arrontime.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoqDLN2AGU4f"
      },
      "outputs": [],
      "source": [
        "query = \"SELECT DepDelay,COUNT(*)as N FROM flights_data WHERE ArrDelay<15 GROUP BY DepDelay\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "arrontime =flight_arrontime.toPandas()\n",
        "arrontime.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZY-0HQ13yOq"
      },
      "outputs": [],
      "source": [
        "\n",
        "query = \"SELECT UniqueCarrier,COUNT(*) as N  FROM flights_data WHERE Arrdelay<15  GROUP BY UniqueCarrier ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_arrontime = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "arrontime = flight_arrontime.toPandas()\n",
        "arrontime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xiLqB4_LYpL"
      },
      "outputs": [],
      "source": [
        "# Total number of cancelled flights\n",
        "query = \"SELECT COUNT(*) FROM flights_data WHERE Cancelled<>0\"\n",
        "\n",
        "# Run the query\n",
        "flight_cancelled = spark.sql(query)\n",
        "cancelled =flight_cancelled.toPandas()\n",
        "cancelled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwY9qOhVZpla"
      },
      "outputs": [],
      "source": [
        "query = \"SELECT Origin,Dest,COUNT(*) as N  FROM flights_data WHERE Depdelay<15  GROUP BY Origin,Dest ORDER BY N DESC\"\n",
        "\n",
        "# Run the query\n",
        "flight_depontime = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "depontime = flight_depontime.toPandas()\n",
        "depontime.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOH_jtA04l4t"
      },
      "outputs": [],
      "source": [
        "# Cast the columns to integers\n",
        "data = data.withColumn(\"Month\", data.Month.cast(\"integer\"))\n",
        "data = data.withColumn(\"DayOfWeek\", data.DayOfWeek.cast(\"integer\"))\n",
        "data = data.withColumn(\"AirTime\", data.AirTime.cast(\"integer\"))\n",
        "data = data.withColumn(\"Distance\", data.Distance.cast(\"double\"))\n",
        "data = data.withColumn(\"ArrDelay\", data.ArrDelay.cast(\"integer\"))\n",
        "data = data.withColumn(\"Cancelled\", data.Cancelled.cast(\"double\"))\n",
        "data = data.withColumn(\"DepTime\", data.CRSDepTime.cast(\"double\"))\n",
        "data = data.withColumn(\"ArrTime\", data.CRSArrTime.cast(\"double\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDdBsZkjvcvY"
      },
      "outputs": [],
      "source": [
        "# filtering columns for our model\n",
        "data_model = data.select('Month', 'DayOfWeek', 'UniqueCarrier', 'TailNum', 'Dest', 'AirTime', 'Distance', 'ArrDelay')\n",
        "\n",
        "# Removing missing values/ Data Cleaning\n",
        "data_model = data_model.filter(\"ArrDelay is not NULL and Distance is not NULL and AirTime is not NULL and  DepDelay is not NULL \")\n",
        "\n",
        "# rows left\n",
        "data_model.count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select the relevant columns for correlation analysis\n",
        "cols = [\"Month\", \"Cancelled\", \"ArrTime\", \"DepTime\", \"DayOfWeek\",\n",
        "        \"AirTime\",\"Distance\",\"ArrDelay\"]\n",
        "\n",
        "# Compute the correlation matrix for the selected columns\n",
        "corr_matrix = data.select(cols).toPandas().corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"YlGnBu\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lFQjSlGtLYlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgMeg3i88D57"
      },
      "outputs": [],
      "source": [
        "# Create is_late (label) for our model\n",
        "data_model = data_model.withColumn(\"is_late\", data_model.ArrDelay > 15)\n",
        "\n",
        "# casting to integer values\n",
        "data_model = data_model.withColumn(\"is_late\", data_model.is_late.cast(\"integer\"))\n",
        "\n",
        "# renaming the column\n",
        "data_model = data_model.withColumnRenamed(\"is_late\", 'label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvRPOFPb8U8O"
      },
      "outputs": [],
      "source": [
        "data_model.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMXBCXoM6v-r"
      },
      "outputs": [],
      "source": [
        "data_model.groupBy('label').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujTPVj2V8vpg"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "\n",
        "# Create a StringIndexer for our pipeline\n",
        "airline_indexer = StringIndexer(inputCol=\"UniqueCarrier\", outputCol=\"airline_index\")\n",
        "\n",
        "# Create a OneHotEncoder to map a column\n",
        "airline_encoder = OneHotEncoder(inputCol=\"airline_index\", outputCol=\"airline_fact\")\n",
        "\n",
        "# Create a StringIndexer\n",
        "tail_indexer = StringIndexer(inputCol=\"TailNum\", outputCol=\"tail_index\")\n",
        "\n",
        "# Create a OneHotEncoder\n",
        "tail_encoder = OneHotEncoder(inputCol=\"tail_index\", outputCol=\"tail_fact\")\n",
        "\n",
        "# Create a StringIndexer\n",
        "dest_indexer = StringIndexer(inputCol=\"Dest\", outputCol=\"dest_index\")\n",
        "\n",
        "# Create a OneHotEncoder\n",
        "dest_encoder = OneHotEncoder(inputCol=\"dest_index\", outputCol=\"dest_fact\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFIDa8qV9rIB"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "#This Transformer takes all of the columns you specify and combines them into a new vector column.\n",
        "\n",
        "# Make a VectorAssembler\n",
        "vec_assembler = VectorAssembler(inputCols=[\"Month\", \"DayOfWeek\", \"AirTime\", \"Distance\", \"airline_fact\", \"dest_fact\", \"tail_fact\"], outputCol=\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35bAeY2V-Wo6"
      },
      "outputs": [],
      "source": [
        "# Import Pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Create the pipeline\n",
        "flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, airline_indexer, airline_encoder, tail_indexer, tail_encoder, vec_assembler])\n",
        "\n",
        "piped_data = flights_pipe.fit(data_model).transform(data_model)\n",
        "\n",
        "train_data, test_data = piped_data.randomSplit([.7, .3])#Splitting training and testing data into 70% and 30%\n",
        "\n",
        "print('data points(rows) in train data :',  train_data.count())\n",
        "print('data points(rows) in test data :',  test_data.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing LogisticRegression\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Creating a LogisticRegression Estimator\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Call model.fit()\n",
        "best_model = model.fit(train_data)\n",
        "\n",
        "# Print best_lr\n",
        "print(best_model)"
      ],
      "metadata": {
        "id": "alPS-GH64JKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r844WljAFI-L"
      },
      "outputs": [],
      "source": [
        "# Use the model to predict the test set\n",
        "test_results = best_model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-79tuIlaG6sQ"
      },
      "outputs": [],
      "source": [
        "test_results.select('label', 'prediction').show(20, False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoiuqtE7NHAw"
      },
      "outputs": [],
      "source": [
        "# Create a confusion matrix\n",
        "test_results.groupBy('label', 'prediction').count().show()\n",
        "\n",
        "# Calculate the elements of the confusion matrix\n",
        "TN = test_results.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = test_results.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = test_results.filter('prediction = 0 AND label != prediction').count()\n",
        "FP = test_results.filter('prediction = 1 AND label != prediction').count()\n",
        "\n",
        "# Accuracy measures the proportion of correct predictions\n",
        "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4cF0SCpiCl1"
      },
      "outputs": [],
      "source": [
        "# Create a DT classifier object and fit to the training data\n",
        "tree = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "tree_model = tree.fit(train_data)\n",
        "# Create predictions on test data\n",
        "test_results = tree_model.transform(test_data)\n",
        "test_results.select('label', 'prediction').show(20, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJTDfQWIii1P"
      },
      "outputs": [],
      "source": [
        "# Create a confusion matrix\n",
        "test_results.groupBy('label', 'prediction').count().show()\n",
        "\n",
        "# Calculate the elements of the confusion matrix\n",
        "TN = test_results.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = test_results.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = test_results.filter('prediction = 0 AND label != prediction').count()\n",
        "FP = test_results.filter('prediction = 1 AND label != prediction').count()\n",
        "\n",
        "# Accuracy measures the proportion of correct predictions\n",
        "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the RDDs to NumPy arrays\n",
        "train_features = np.array(train_data.select(\"features\").rdd.map(lambda x: x[0]).collect())\n",
        "train_labels = np.array(train_data.select(\"label\").rdd.map(lambda x: x[0]).collect())\n",
        "test_features = np.array(test_data.select(\"features\").rdd.map(lambda x: x[0]).collect())\n",
        "test_labels = np.array(test_data.select(\"label\").rdd.map(lambda x: x[0]).collect())\n",
        "\n"
      ],
      "metadata": {
        "id": "gLcWpw2mVr6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model.fit(train_features, train_labels, epochs=10, batch_size=64, validation_data=(test_features, test_labels))\n"
      ],
      "metadata": {
        "id": "zSL_eWkZWD7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_labels)\n"
      ],
      "metadata": {
        "id": "hnCQEkYtWLvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test data\n",
        "predictions = model.predict(test_features)\n",
        "\n",
        "# Convert predictions to integer class labels\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print the predicted labels\n",
        "print(predicted_labels)\n"
      ],
      "metadata": {
        "id": "JgbeW7C0X6ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the test labels and predicted labels to a pandas dataframe\n",
        "results_df = pd.DataFrame({'test_labels': test_labels, 'predicted_labels': predicted_labels})\n",
        "\n",
        "# Convert the pandas dataframe to a Spark DataFrame\n",
        "results_spark = spark.createDataFrame(results_df)\n",
        "\n",
        "# Show the first 10 rows of the results DataFrame\n",
        "results_spark.show(10)\n"
      ],
      "metadata": {
        "id": "v3oroJOEYpCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions = model.predict(test_features)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print the predicted labels\n",
        "print(predicted_labels)\n",
        "\n",
        "# Generate classification report\n",
        "target_names = ['Class 0', 'Class 1']\n",
        "print(classification_report(test_labels, predicted_labels, target_names=target_names))\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_mat = confusion_matrix(test_labels, predicted_labels)\n",
        "print(conf_mat)\n",
        "\n",
        "# Generate heatmap\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.imshow(conf_mat, cmap=plt.cm.Blues)\n",
        "ax.set_xticks(np.arange(len(target_names)))\n",
        "ax.set_yticks(np.arange(len(target_names)))\n",
        "ax.set_xticklabels(target_names)\n",
        "ax.set_yticklabels(target_names)\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "for i in range(len(target_names)):\n",
        "    for j in range(len(target_names)):\n",
        "        ax.text(j, i, conf_mat[i, j], ha=\"center\", va=\"center\", color=\"white\")\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cE8VAdHfbZfS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}